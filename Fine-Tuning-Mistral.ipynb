{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8609527,"sourceType":"datasetVersion","datasetId":5143717}],"dockerImageVersionId":30716,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! rm -rf /opt/conda/lib/python3.10/site-packages/aiohttp-3.9.1.dist-info","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-05T05:32:52.061855Z","iopub.execute_input":"2024-06-05T05:32:52.062639Z","iopub.status.idle":"2024-06-05T05:32:53.050854Z","shell.execute_reply.started":"2024-06-05T05:32:52.062605Z","shell.execute_reply":"2024-06-05T05:32:53.049461Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install -qU transformers datasets accelerate bitsandbytes peft trl accelerate langchain_core langchain flash_attn","metadata":{"execution":{"iopub.status.busy":"2024-06-05T05:32:53.053333Z","iopub.execute_input":"2024-06-05T05:32:53.053706Z","iopub.status.idle":"2024-06-05T05:33:49.552259Z","shell.execute_reply.started":"2024-06-05T05:32:53.053671Z","shell.execute_reply":"2024-06-05T05:33:49.551113Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.4.1 requires cubinlinker, which is not installed.\ncudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.4.1 requires ptxcompiler, which is not installed.\ncuml 24.4.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.9.0 requires keras-core, which is not installed.\nkeras-nlp 0.12.1 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 14.0.2 which is incompatible.\nbeatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.2.1 which is incompatible.\ncudf 24.4.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ndistributed 2024.1.1 requires dask==2024.1.1, but you have dask 2024.5.1 which is incompatible.\ngoogle-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\njupyterlab 4.2.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.3 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask==2024.1.1, but you have dask 2024.5.1 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask-expr==0.4.0, but you have dask-expr 1.1.1 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom torch import cuda\nimport torch\n\n\ndevice = f'cuda :{cuda.current_device()}' if cuda.is_available() else  'cpu'\nhf_auth = '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'\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,            # load model in 4-bit precision\n    bnb_4bit_quant_type=\"nf4\",    # pre-trained model should be quantized in 4-bit NF format\n    bnb_4bit_use_double_quant=True, # Using double quantization as mentioned in QLoRA paper\n    bnb_4bit_compute_dtype=torch.bfloat16, # During computation, pre-trained model should be loaded in BF16 format\n)\n\n# Load pre-trained Mistral model and tokenizer\nmodel_name = \"mistralai/Mistral-7B-v0.3\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, use_auth_token=hf_auth, quantization_config=bnb_config,\n                                             device_map=\"auto\", use_cache=False )\ntokenizer = AutoTokenizer.from_pretrained(model_name , use_auth_token=hf_auth , trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"execution":{"iopub.status.busy":"2024-06-05T05:33:49.553964Z","iopub.execute_input":"2024-06-05T05:33:49.554901Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/601 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00d09453c21e47f8af73455d86f95a49"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"461308812a6148868befd782a3bc1fb9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14ece00d81cf4cba8cdc1e52bfd2348e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"375c012a8a1b4edb912f49cef52933ac"}},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd \npath = '/kaggle/input/testcsv/documents.csv'\ndf = pd.read_csv(path)\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"page_content\"], padding=\"max_length\", truncation=True)\n\n# Convert your DataFrame to a Dataset\nfrom datasets import Dataset\ndataset = Dataset.from_pandas(df)\n\n# Map the tokenization function to the dataset\ntokenized_dataset = dataset.map(tokenize_function, batched=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\nimport bitsandbytes as bnb\n\n# Define LoRA configuration\nlora_config = LoraConfig(\n    r=8,  # Rank of the low-rank adaptation\n    lora_alpha=32,  # Scaling factor\n    lora_dropout=0.02,  # Dropout rate for LoRA\n    bias=\"none\",  # setting to 'none' for only training weight params instead of biases\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = get_peft_model(model, lora_config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Freeze all layers except LoRA layers\nfor name, param in model.named_parameters():\n    if 'lora_' not in name:\n        param.requires_grad = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\nfrom trl import SFTTrainer\n\ncuda.empty_cache()\ntraining_params = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=1,     # Number of training epochs\n    per_device_train_batch_size=1,  # Batch size for training \n    per_device_eval_batch_size=1,   # Batch size for evaluation        \n    warmup_steps=500,              # Number of warmup steps (optional)\n    weight_decay=0.01,\n    save_steps=10,\n    fp16=True,                     # Use mixed precision\n    push_to_hub=False,\n    save_strategy=\"epoch\",        # Save checkpoint after each epoch\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=tokenized_dataset,\n    tokenizer=tokenizer,\n    args=training_params,\n    dataset_text_field=\"text\"  # This field corresponds to the column containing text data\n)\ntrainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain_core.prompts import PromptTemplate\nfrom langchain.prompts import ChatPromptTemplate\n\nprompt = PromptTemplate(\n    template=\"{question}\", input_variables=[\"question\"],)\n\ndef generate_text(query_text : str , model, tokenizer):\n    \n    prompt_text = prompt.format(question=query_text)\n    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(model.device)\n    \n    outputs = model.generate(\n        **inputs,\n        max_length=10000,\n        num_return_sequences=1,\n        no_repeat_ngram_size=2,\n        early_stopping=True\n    )\n    \n    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    formatted_response = f\"\"\"\n    --> Assistant Response\n\n    -->✨ User Question:\n    {query_text}\n\n    -->✅ Answer:\n    \n    {text} \n\n    \"\"\"\n    print(formatted_response)\n    return text\n\nquestion = 'quel sont les formations de la facultédecrire la Formation continue / DCA Ingénierie topographique et systèmes de information géographiques appliqués'\n\ngenerate_text(question, model, tokenizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}